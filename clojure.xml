<?xml version='1.0' encoding='UTF-8'?>
<rss version='2.0' xmlns:atom='http://www.w3.org/2005/Atom'>
<channel>
<atom:link href='https://bentomi.github.io/' rel='self' type='application/rss+xml'/>
<title>
Bentomi's Notepad
</title>
<link>
https://bentomi.github.io/
</link>
<description>
Notes made during my experiments
</description>
<lastBuildDate>
Sun, 22 Aug 2021 13:02:57 +0200
</lastBuildDate>
<generator>
clj-rss
</generator>
<item>
<guid>
https://bentomi.github.io/posts-output/2021-08-22-binding-conveyance-custom-executor-part-iii/
</guid>
<link>
https://bentomi.github.io/posts-output/2021-08-22-binding-conveyance-custom-executor-part-iii/
</link>
<title>
Custom executors and binding conveyance in Clojure, part III
</title>
<description>
&lt;p&gt;In the &lt;a href='/posts-output/2021-08-13-binding-conveyance-custom-executor-part-ii/'&gt;previous post&lt;/a&gt;, I described how to set up dynamic variable bindings for arbitrary executors. I also pointed out that this method has a slight performance penalty: the bindings are set up and torn down before and after each submitted task, although we need this only once for each thread in the pool.&lt;/p&gt;&lt;p&gt;In this part I show a way to get rid of this performance penalty whenever using an executor accepting a thread factory. The following examples assume the environment described in the &lt;a href='/posts-output/2021-08-09-binding-conveyance-custom-executor/'&gt;first part&lt;/a&gt; of this series.&lt;/p&gt;&lt;h1 id=&quot;a&amp;#95;thread&amp;#95;factory&amp;#95;with&amp;#95;dynamic&amp;#95;binding&quot;&gt;A thread factory with dynamic binding&lt;/h1&gt;&lt;p&gt;The first step is to define a function a thread factory creating threads with the desired bindings already installed. The function below takes two optional parameters, a thread factory producing the threads we want to modify and the bindings to be installed. Both of these have straightforward default values: the default thread factory from &lt;code&gt;java.util.concurrent.Executors&lt;/code&gt; and the current thread bindings.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;clojure&quot;&gt;&amp;#40;defn binding-thread-factory
  &amp;quot;Returns a thread factory wrapping `base-factory` that creates threads having
  `bindings` installed.&amp;quot;
  &amp;#91;&amp;amp; {:keys &amp;#91;base-factory bindings&amp;#93;
      :or {base-factory &amp;#40;Executors/defaultThreadFactory&amp;#41;
           bindings &amp;#40;get-thread-bindings&amp;#41;}}&amp;#93;
  &amp;#40;reify ThreadFactory
    &amp;#40;newThread &amp;#91;&amp;#95;this runnable&amp;#93;
      &amp;#40;.newThread base-factory #&amp;#40;with-bindings bindings &amp;#40;.run runnable&amp;#41;&amp;#41;&amp;#41;&amp;#41;&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the last line of this code the &lt;code&gt;with-bindings&lt;/code&gt; macro is used to set the desired bindings for the runnable of the new thread.&lt;/p&gt;&lt;p&gt;This function can be used as shown below for creating a fixed thread pool executor.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;clojure&quot;&gt;&amp;#40;defn &amp;#94;ExecutorService binding-fixed-thread-pool
  &amp;quot;Returns a fixed thread pool with `threads` number of threads using a binding
  thread pool created according to `factory-opts`.
  Also see: `binding-thread-factory`.&amp;quot;
  &amp;#91;threads &amp;amp; factory-opts&amp;#93;
  &amp;#40;let &amp;#91;thread-factory &amp;#40;apply binding-thread-factory factory-opts&amp;#41;&amp;#93;
    &amp;#40;Executors/newFixedThreadPool threads thread-factory&amp;#41;&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&quot;using&amp;#95;the&amp;#95;executor&amp;#95;in&amp;#95;tests&quot;&gt;Using the executor in tests&lt;/h1&gt;&lt;p&gt;Like before, it makes sense to define a macro for creating and shutting down the executor:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;clojure&quot;&gt;&amp;#40;spec/fdef with-executor
  :args &amp;#40;spec/cat :binding &amp;#40;spec/spec &amp;#40;spec/cat :name simple-symbol?
                                                :executor any?&amp;#41;&amp;#41;
                  :body &amp;#40;spec/+ any?&amp;#41;&amp;#41;&amp;#41;

&amp;#40;defmacro with-executor
  &amp;quot;Creates an ExecutorService by calling `executor` and executes `body`.
  The executor service created is bound to `name` and shut down after the
  execution of `body`.&amp;quot;
  &amp;#91;&amp;#91;name executor&amp;#93; &amp;amp; body&amp;#93;
  `&amp;#40;let &amp;#91;&amp;#126;name &amp;#126;executor&amp;#93;
     &amp;#40;try
       &amp;#126;@body
       &amp;#40;finally
         &amp;#40;.shutdown &amp;#126;name&amp;#41;&amp;#41;&amp;#41;&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With this macro a multi-threaded test can be defined such:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;clojure&quot;&gt;&amp;#40;deftest multi-thread-bind-once
  &amp;#40;let &amp;#91;threads 8, tasks &amp;#40;&amp;#42; 2 threads&amp;#41;&amp;#93;
    &amp;#40;with-executor &amp;#91;executor &amp;#40;binding-fixed-thread-pool threads&amp;#41;&amp;#93;
      &amp;#40;-&amp;gt;&amp;gt; &amp;#40;repeatedly tasks #&amp;#40;.submit executor &amp;#94;Callable sut-check&amp;#41;&amp;#41;
           doall
           &amp;#40;map #&amp;#40;try &amp;#40;.get % 1 TimeUnit/SECONDS&amp;#41;
                      &amp;#40;catch TimeoutException &amp;#95; ::timeout&amp;#41;&amp;#41;&amp;#41;
           &amp;#40;every? true?&amp;#41;
           is&amp;#41;&amp;#41;&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;See the &lt;a href='https://github.com/bentomi/conveyance'&gt;source code&lt;/a&gt; if you want to play with it.&lt;/p&gt;
</description>
<pubDate>
Sun, 22 Aug 2021 00:00:00 +0200
</pubDate>
</item>
<item>
<guid>
https://bentomi.github.io/posts-output/2021-08-13-binding-conveyance-custom-executor-part-ii/
</guid>
<link>
https://bentomi.github.io/posts-output/2021-08-13-binding-conveyance-custom-executor-part-ii/
</link>
<title>
Custom executors and binding conveyance in Clojure, part II
</title>
<description>
&lt;p&gt;In the &lt;a href='/posts-output/2021-08-09-binding-conveyance-custom-executor/'&gt;previous post&lt;/a&gt;, I described how the agents' send-off pool can be overridden to ensure binding conveyance for calls running on a thread different from that of the caller.&lt;/p&gt;&lt;p&gt;This method has good performance but comes with limitations: it obviously interferes with agents getting tasks with &lt;code&gt;send-off&lt;/code&gt; and can cause problems if we want to run tasks in the thread pool that should not see the bindings in the caller's environment. Also, it relies on internal code.&lt;/p&gt;&lt;h1 id=&quot;using&amp;#95;the&amp;#95;public&amp;#95;api&quot;&gt;Using the public API&lt;/h1&gt;&lt;p&gt;If performance is not an issue or the limitations are unacceptable, it is better to use the public &lt;code&gt;bound-fn&lt;/code&gt; macro or &lt;code&gt;bound-fn&amp;#42;&lt;/code&gt; functions. These return a function that installs the bindings of their caller before executing their arguments and clean them up afterwards. In other words, the bindings are pushed and popped every time the function is executed.&lt;/p&gt;&lt;p&gt;With a small change (see line number 3 below) we can fix the &lt;code&gt;multi-thread-naive&lt;/code&gt; test:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;clojure&quot;&gt;&amp;#40;deftest multi-thread-bound-fn
  &amp;#40;let &amp;#91;threads 8, tasks &amp;#40;&amp;#42; 2 threads&amp;#41;
        &amp;#94;Callable sut-check &amp;#40;bound-fn&amp;#42; sut-check&amp;#41;
        executor &amp;#40;Executors/newFixedThreadPool threads&amp;#41;&amp;#93;
    &amp;#40;try
      &amp;#40;-&amp;gt;&amp;gt; &amp;#40;repeatedly tasks #&amp;#40;.submit executor sut-check&amp;#41;&amp;#41;
           doall
           &amp;#40;map #&amp;#40;try &amp;#40;.get % 1 TimeUnit/SECONDS&amp;#41;
                      &amp;#40;catch TimeoutException &amp;#95; ::timeout&amp;#41;&amp;#41;&amp;#41;
           &amp;#40;every? true?&amp;#41;
           is&amp;#41;
      &amp;#40;finally
        &amp;#40;.shutdown executor&amp;#41;&amp;#41;&amp;#41;&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The &lt;code&gt;sut-check&lt;/code&gt; function is replaced with another function installing the bindings by calling &lt;code&gt;bound-fn&amp;#42;&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;You can have a look at the &lt;a href='https://github.com/bentomi/conveyance'&gt;source code&lt;/a&gt; if you want to play with it.&lt;/p&gt;
</description>
<pubDate>
Fri, 13 Aug 2021 00:00:00 +0200
</pubDate>
</item>
<item>
<guid>
https://bentomi.github.io/posts-output/2021-08-09-binding-conveyance-custom-executor/
</guid>
<link>
https://bentomi.github.io/posts-output/2021-08-09-binding-conveyance-custom-executor/
</link>
<title>
Custom executors and binding conveyance in Clojure
</title>
<description>
&lt;p&gt;Every now and then I run into a situation when I need to execute code in multiple threads in a controlled way. A typical example for such a situation is when I want to test my code for thread safety or performance under various concurrency settings.&lt;/p&gt;&lt;p&gt;The JVM offers a convenient way for doing this: the &lt;a href='https://docs.oracle.com/en/java/javase/15/docs/api/java.base/java/util/concurrent/Executors.html'&gt;&lt;code&gt;java.util.concurrent.Executors&lt;/code&gt;&lt;/a&gt; class provides a number of useful predefined &lt;code&gt;ExecutorService&lt;/code&gt; types with different concurrency properties. However, using them in Clojure properly is not entirely trivial, as in many cases dynamic variable bindings have to be taken into account.&lt;/p&gt;&lt;h1 id=&quot;a&amp;#95;simple&amp;#95;test&amp;#95;case&quot;&gt;A simple test case&lt;/h1&gt;&lt;p&gt;To illustrate the point, let's imagine that we have a function that should work right even when invoked from concurrent threads. As an (admittedly contrived) example, the function &lt;code&gt;sut&lt;/code&gt; below has the task of setting the value in the &lt;code&gt;global&lt;/code&gt; atom to the next higher value divisible by &lt;code&gt;modulus&lt;/code&gt;.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;clojure&quot;&gt;&amp;#40;def modulus 97&amp;#41;

&amp;#40;def global &amp;#40;atom 0&amp;#41;&amp;#41;

&amp;#40;defn sut &amp;#91;&amp;#93;
  &amp;#40;let &amp;#91;c @global&amp;#93;
    &amp;#40;Thread/sleep &amp;#40;inc &amp;#40;rand-int 20&amp;#41;&amp;#41;&amp;#41;
    &amp;#40;let &amp;#91;m &amp;#40;mod c modulus&amp;#41;
          result &amp;#40;if &amp;#40;pos? m&amp;#41;
                   &amp;#40;swap! global + &amp;#40;- modulus m&amp;#41;&amp;#41;
                   c&amp;#41;&amp;#93;
      &amp;#40;when &amp;#40;pos? &amp;#40;mod @global modulus&amp;#41;&amp;#41;
        &amp;#40;case &amp;#40;rand-int 200&amp;#41;
          0 &amp;#40;throw &amp;#40;ex-info &amp;quot;error detected&amp;quot; {}&amp;#41;&amp;#41;
          1 &amp;#40;Thread/sleep 60000&amp;#41;
          nil&amp;#41;&amp;#41;
      result&amp;#41;&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This function is obviously not thread safe: reading and writing the global variable is not done atomically. Sleeping between reading and writing the global variable makes it very likely that in the presence of concurrent calls the end value will not be divisible by &lt;code&gt;modulus&lt;/code&gt;. The function also simulates some common erroneous behaviour: whenever it sees that adjusting the global value failed, with a small probability it throws an exception or just hangs for a long time.&lt;/p&gt;&lt;p&gt;Ideally, we want our tests to explicitly report all of these problems, when the state gets an invalid value, when the computation fails and when the computation blocks.&lt;/p&gt;&lt;h1 id=&quot;testing&amp;#95;from&amp;#95;a&amp;#95;single&amp;#95;thread&quot;&gt;Testing from a single thread&lt;/h1&gt;&lt;p&gt;We define a fixture that sets &lt;code&gt;global&lt;/code&gt; to a random value between 0 and 9999 before a test is executed. The test itself makes two checks. First, if the result of &lt;code&gt;sut&lt;/code&gt; is divisible with &lt;code&gt;modulus&lt;/code&gt; and second, if the result is a natural integer.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;clojure&quot;&gt;&amp;#40;test/use-fixtures :each &amp;#40;fn &amp;#91;f&amp;#93;
                           &amp;#40;reset! global &amp;#40;rand-int 10000&amp;#41;&amp;#41;
                           &amp;#40;f&amp;#41;&amp;#41;&amp;#41;

&amp;#40;defn sut-check &amp;#91;&amp;#93;
  &amp;#40;is &amp;#40;zero? &amp;#40;mod &amp;#40;sut&amp;#41; modulus&amp;#41;&amp;#41;&amp;#41;
  &amp;#40;is &amp;#40;nat-int? &amp;#40;sut&amp;#41;&amp;#41;&amp;#41;&amp;#41;

&amp;#40;deftest single-thread
  &amp;#40;sut-check&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The test &lt;code&gt;single-thread&lt;/code&gt; always passes.&lt;/p&gt;&lt;h1 id=&quot;multi-threaded&amp;#95;testing&amp;#95;the&amp;#95;naive&amp;#95;way&quot;&gt;Multi-threaded testing the naive way&lt;/h1&gt;&lt;p&gt;The test below calls the same function doing the checks as the single threaded one, but starts multiple concurrent calls.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;clojure&quot;&gt;&amp;#40;deftest multi-thread-naive
  &amp;#40;let &amp;#91;threads 8, tasks &amp;#40;&amp;#42; 2 threads&amp;#41;
        executor &amp;#40;Executors/newFixedThreadPool threads&amp;#41;&amp;#93;
    &amp;#40;try
      &amp;#40;-&amp;gt;&amp;gt; &amp;#40;repeatedly tasks #&amp;#40;.submit executor &amp;#94;Callable sut-check&amp;#41;&amp;#41;
           doall
           &amp;#40;map #&amp;#40;try &amp;#40;.get % 1 TimeUnit/SECONDS&amp;#41;
                      &amp;#40;catch TimeoutException &amp;#95; ::timeout&amp;#41;&amp;#41;&amp;#41;
           &amp;#40;every? true?&amp;#41;
           is&amp;#41;
      &amp;#40;finally
        &amp;#40;.shutdown executor&amp;#41;&amp;#41;&amp;#41;&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It first creates an executor with a pool of 8 threads, then submits 16 tasks to it. &lt;code&gt;repeatedly&lt;/code&gt; delivers a lazy sequence, so the submissions are forced with &lt;code&gt;doall&lt;/code&gt;. This results in a list of futures and we get their values with a &lt;code&gt;map&lt;/code&gt; call. In case we do not get the result within a second, we return &lt;code&gt;::timeout&lt;/code&gt;. (We know that &lt;code&gt;sut-check&lt;/code&gt; cannot deliver this value, so it unambiguously identifies a timeout.) Then we check that the result of each task is &lt;code&gt;true&lt;/code&gt;, which is the value the &lt;code&gt;is&lt;/code&gt; macro delivers when its assertion holds. Finally, we shut down the executor.&lt;/p&gt;&lt;p&gt;The output of running this test depends on the test runner and the environment. Running this test with &lt;code&gt;cognitect.test-runner&lt;/code&gt; in a shell produces an output like this:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;shell&quot;&gt;$ clojure -X:run :vars '&amp;#91;com.github.bentomi.demo-test/multi-thread-naive&amp;#93;'

Running tests in #{&amp;quot;test&amp;quot;}

Testing com.github.bentomi.demo-test

FAIL in
FAIL in  &amp;#40;&amp;#41; &amp;#40;demo&amp;#95;test.clj:49&amp;#41;
FAIL in

FAIL in
FAIL in&amp;#40;&amp;#41; &amp;#40;demo&amp;#95;test.clj:49&amp;#41;
&amp;#40;&amp;#41; &amp;#40;demo&amp;#95;test.clj:49&amp;#41;
&amp;#40;&amp;#41; &amp;#40;demo&amp;#95;test.clj:49&amp;#41;
&amp;#40;&amp;#41; &amp;#40;demo&amp;#95;test.clj:49&amp;#41;
FAIL in

FAIL in &amp;#40;&amp;#41; &amp;#40;demo&amp;#95;test.clj:49&amp;#41;
 &amp;#40;&amp;#41; &amp;#40;demo&amp;#95;test.clj:49&amp;#41;
expected: expected:&amp;#40;zero? &amp;#40;mod &amp;#40;sut&amp;#41; modulus&amp;#41;&amp;#41;
expected: &amp;#40;zero? &amp;#40;mod &amp;#40;sut&amp;#41; modulus&amp;#41;&amp;#41;
&amp;#40;zero? &amp;#40;mod &amp;#40;sut&amp;#41; modulus&amp;#41;&amp;#41;
expected: expected:&amp;#40;zero? &amp;#40;mod &amp;#40;sut&amp;#41; modulus&amp;#41;&amp;#41;
&amp;#40;zero? &amp;#40;mod &amp;#40;sut&amp;#41; modulus&amp;#41;&amp;#41;
expected:expected:  &amp;#40;zero? &amp;#40;mod &amp;#40;sut&amp;#41; modulus&amp;#41;&amp;#41;&amp;#40;zero? &amp;#40;mod &amp;#40;sut&amp;#41; modulus&amp;#41;&amp;#41;

  actual:  actual:  &amp;#40;not &amp;#40;zero? 1&amp;#41;&amp;#41;
  actual: &amp;#40;not &amp;#40;zero? 33&amp;#41;&amp;#41;&amp;#40;not &amp;#40;zero? 65&amp;#41;&amp;#41;

  actual: &amp;#40;not &amp;#40;zero? 66&amp;#41;&amp;#41;
  actual: &amp;#40;not &amp;#40;zero? 2&amp;#41;&amp;#41;
  actual:  actual:  &amp;#40;not &amp;#40;zero? 34&amp;#41;&amp;#41;&amp;#40;not &amp;#40;zero? 34&amp;#41;&amp;#41;


Ran 1 tests containing 1 assertions.
0 failures, 0 errors.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can see some garbled error messages and the report at the end that test passed with one assertion. This is because the assertion in &lt;code&gt;multi-thread-naive&lt;/code&gt; can only detect when &lt;code&gt;sut-check&lt;/code&gt; takes longer than a second or when the second call to &lt;code&gt;is&lt;/code&gt; throws an exception. In the first case the corresponding value is &lt;code&gt;::timeout&lt;/code&gt;, in the second case it's &lt;code&gt;nil&lt;/code&gt;. As the likelihood of these failures is low, the test passes most of the time. In an IDE the output is often hidden, and we can only see that test passes.&lt;/p&gt;&lt;p&gt;The assertions in &lt;code&gt;sut-check&lt;/code&gt; are not taken into account in the report, because their results are collected in a Ref stored in the &lt;code&gt;clojure.test/&amp;#42;report-counters&amp;#42;&lt;/code&gt; dynamic variable which is not seen in the threads making the calls.&lt;/p&gt;&lt;p&gt;Fortunately, Clojure supports binding conveyance, that is, if you make a call using &lt;code&gt;future&lt;/code&gt;, the call will be executed in another thread but it will still see the bindings existing in the current thread.&lt;/p&gt;&lt;h1 id=&quot;multi-threaded&amp;#95;testing&amp;#95;with&amp;#95;binding&amp;#95;conveyance&quot;&gt;Multi-threaded testing with binding conveyance&lt;/h1&gt;&lt;p&gt;The functions &lt;code&gt;future&lt;/code&gt; and &lt;code&gt;future-call&lt;/code&gt; execute their arguments with the same executor that is used with agents when their task is submitted with &lt;code&gt;send-off&lt;/code&gt;. As discussed in &lt;a href='/posts-output/2017-06-04-message-sending-clojure-elixir/'&gt;an earlier post&lt;/a&gt;, the function &lt;code&gt;set-agent-send-off-executor!&lt;/code&gt; can be used to set our custom executor for use by &lt;code&gt;send-off&lt;/code&gt;, &lt;code&gt;future&lt;/code&gt; and &lt;code&gt;future-call&lt;/code&gt;. This means that, as long as the code we are testing is not using any functions relying on this executor, we can override it for the scope of the test. Unfortunately, &lt;code&gt;set-agent-send-off-executor!&lt;/code&gt; returns the executor we set, not the original one, so we have to read &lt;code&gt;clojure.lang.Agent/soloExecutor&lt;/code&gt; explicitly.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;clojure&quot;&gt;&amp;#40;deftest multi-thread-conveying
  &amp;#40;let &amp;#91;threads 8, tasks &amp;#40;&amp;#42; 2 threads&amp;#41;
        executor &amp;#40;Executors/newFixedThreadPool threads&amp;#41;
        original-executor clojure.lang.Agent/soloExecutor&amp;#93;
    &amp;#40;set-agent-send-off-executor! executor&amp;#41;
    &amp;#40;try
      &amp;#40;-&amp;gt;&amp;gt;
       &amp;#40;repeatedly tasks #&amp;#40;future-call sut-check&amp;#41;&amp;#41;
       doall
       &amp;#40;map #&amp;#40;deref % 1000 ::timeout&amp;#41;&amp;#41;
       &amp;#40;every? true?&amp;#41;
       is&amp;#41;
      &amp;#40;finally
        &amp;#40;set-agent-send-off-executor! original-executor&amp;#41;
        &amp;#40;.shutdown executor&amp;#41;&amp;#41;&amp;#41;&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This version of the test differs from the naive version only in that it installs our executor as the agent send off executor for the scope of the test and instead of dealing with Java's Futures, it uses Clojure's &lt;code&gt;deref&lt;/code&gt; to obtain the result values.&lt;/p&gt;&lt;p&gt;Running this test produces an output like this:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;shell&quot;&gt;$ clojure -X:run :vars '&amp;#91;com.github.bentomi.demo-test/multi-thread-conveying&amp;#93;'

Running tests in #{&amp;quot;test&amp;quot;}

Testing com.github.bentomi.demo-test

FAIL in &amp;#40;multi-thread-conveying&amp;#41; &amp;#40;demo&amp;#95;test.clj:29&amp;#41;

FAIL in &amp;#40;multi-thread-conveying&amp;#41; &amp;#40;demo&amp;#95;test.clj:29&amp;#41;
FAIL in
 &amp;#40;multi-thread-conveying&amp;#41; &amp;#40;demo&amp;#95;test.clj:29&amp;#41;
expected: &amp;#40;zero? &amp;#40;mod &amp;#40;sut&amp;#41; modulus&amp;#41;&amp;#41;
expected: &amp;#40;zero? &amp;#40;mod &amp;#40;sut&amp;#41; modulus&amp;#41;&amp;#41;
expected: &amp;#40;zero? &amp;#40;mod &amp;#40;sut&amp;#41; modulus&amp;#41;&amp;#41;

FAIL in
FAIL in&amp;#40;multi-thread-conveying&amp;#41; &amp;#40;demo&amp;#95;test.clj:29&amp;#41;
&amp;#40;multi-thread-conveying&amp;#41; &amp;#40;demo&amp;#95;test.clj:29&amp;#41;
expected: &amp;#40;zero? &amp;#40;mod &amp;#40;sut&amp;#41; modulus&amp;#41;&amp;#41;
expected: &amp;#40;zero? &amp;#40;mod &amp;#40;sut&amp;#41; modulus&amp;#41;&amp;#41;
  actual: &amp;#40;not &amp;#40;zero? 70&amp;#41;&amp;#41;
  actual:  actual:   actual:&amp;#40;not &amp;#40;zero? 88&amp;#41;&amp;#41;
  &amp;#40;not &amp;#40;zero? 88&amp;#41;&amp;#41;
&amp;#40;not &amp;#40;zero? 79&amp;#41;&amp;#41;
  actual: &amp;#40;not &amp;#40;zero? 79&amp;#41;&amp;#41;

FAIL in &amp;#40;multi-thread-conveying&amp;#41; &amp;#40;demo&amp;#95;test.clj:29&amp;#41;
expected: &amp;#40;zero? &amp;#40;mod &amp;#40;sut&amp;#41; modulus&amp;#41;&amp;#41;
  actual: &amp;#40;not &amp;#40;zero? 70&amp;#41;&amp;#41;

FAIL in &amp;#40;multi-thread-conveying&amp;#41; &amp;#40;demo&amp;#95;test.clj:29&amp;#41;
expected: &amp;#40;zero? &amp;#40;mod &amp;#40;sut&amp;#41; modulus&amp;#41;&amp;#41;
  actual: &amp;#40;not &amp;#40;zero? 43&amp;#41;&amp;#41;

Ran 1 tests containing 33 assertions.
7 failures, 0 errors.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here we can see that there are 33 assertions not just one and, more importantly, that there are seven failures. Now, even if we cannot see the output of the tests, we can notice that they fail.&lt;/p&gt;&lt;h2 id=&quot;reducing&amp;#95;the&amp;#95;boilerplate&quot;&gt;Reducing the boilerplate&lt;/h2&gt;&lt;p&gt;Since it's awkward and error prone setting up the executor like this, we better extract the ceremony into a macro:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;clojure&quot;&gt;&amp;#40;spec/fdef with-send-off-executor
  :args &amp;#40;spec/cat :binding &amp;#40;spec/spec &amp;#40;spec/cat :name simple-symbol?
                                                :executor any?&amp;#41;&amp;#41;
                  :body &amp;#40;spec/+ any?&amp;#41;&amp;#41;&amp;#41;

&amp;#40;defmacro with-send-off-executor
  &amp;quot;Creates an ExecutorService by calling `executor`, sets it for the scope
  of the form as executor for `send-off`, `future`, etc. and executes `body`.
  The executor service created is bound to `name` and shut down after the
  execution of `body`.&amp;quot;
  &amp;#91;&amp;#91;name executor&amp;#93; &amp;amp; body&amp;#93;
  `&amp;#40;let &amp;#91;&amp;#126;name &amp;#126;executor
         original-executor# clojure.lang.Agent/soloExecutor&amp;#93;
     &amp;#40;set-agent-send-off-executor! &amp;#126;name&amp;#41;
     &amp;#40;try
       &amp;#126;@body
       &amp;#40;finally
         &amp;#40;set-agent-send-off-executor! original-executor#&amp;#41;
         &amp;#40;.shutdown &amp;#126;name&amp;#41;&amp;#41;&amp;#41;&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;with-send-off-executor&lt;/code&gt; is symmetrical to Clojure's &lt;code&gt;with-open&lt;/code&gt; macro. It allows us to name the executor created and manipulate it in the body of the form. When the execution leaves the form, the executor is shut down.&lt;/p&gt;&lt;p&gt;Using this macro we can write the test such:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;clojure&quot;&gt;&amp;#40;deftest multi-thread-conveying
  &amp;#40;let &amp;#91;threads 8, tasks &amp;#40;&amp;#42; 2 threads&amp;#41;&amp;#93;
    &amp;#40;with-send-off-executor &amp;#91;&amp;#95;executor &amp;#40;Executors/newFixedThreadPool threads&amp;#41;&amp;#93;
      &amp;#40;-&amp;gt;&amp;gt; &amp;#40;repeatedly tasks #&amp;#40;future-call sut-check&amp;#41;&amp;#41;
           doall
           &amp;#40;map #&amp;#40;deref % 1000 ::timeout&amp;#41;&amp;#41;
           &amp;#40;every? true?&amp;#41;
           is&amp;#41;&amp;#41;&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Have a look at the &lt;a href='https://github.com/bentomi/conveyance'&gt;source code&lt;/a&gt; if you want to play with it.&lt;/p&gt;
</description>
<pubDate>
Mon, 09 Aug 2021 00:00:00 +0200
</pubDate>
</item>
<item>
<guid>
https://bentomi.github.io/posts-output/2017-06-04-message-sending-clojure-elixir/
</guid>
<link>
https://bentomi.github.io/posts-output/2017-06-04-message-sending-clojure-elixir/
</link>
<title>
Message sending with Clojure agents, core.async and Elixir
</title>
<description>
&lt;h2 id=&quot;the&amp;#95;&quot;messages&amp;#95;sent&amp;#95;through&amp;#95;a&amp;#95;chain&quot;&amp;#95;problem&quot;&gt;The &quot;messages sent through a chain&quot; problem&lt;/h2&gt;&lt;p&gt;This is an often used toy problem to benchmark the message passing overhead in asynchronous message passing systems.  We create M number of processes/agents in a chain and send N messages through the whole chain.&lt;/p&gt;&lt;p&gt;I looked at three different implementations of the benchmark, an Elixir version serving as the baseline and two Clojure versions, one with agents and one with core.async.  Here are the basic versions of each.&lt;/p&gt;&lt;h2 id=&quot;elixir&amp;#95;solution&quot;&gt;Elixir solution&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;defmodule Chain do
  def relay&amp;#40;next&amp;#95;pid&amp;#41; do
    receive do
      message -&amp;gt;
        send next&amp;#95;pid, message
        relay&amp;#40;next&amp;#95;pid&amp;#41;
    end
  end

  def create&amp;#95;senders&amp;#40;m&amp;#41; do
    Enum.reduce 1..m, self&amp;#40;&amp;#41;, fn &amp;#40;&amp;#95;, prev&amp;#41; -&amp;gt; spawn&amp;#40;Chain, :relay, &amp;#91;prev&amp;#93;&amp;#41; end
  end

  def test&amp;#40;m, n&amp;#41; do
    message = 0
    start = create&amp;#95;senders&amp;#40;m&amp;#41;
    Enum.each 1..n, fn &amp;#40;&amp;#95;&amp;#41; -&amp;gt; send start, message end
    Enum.each 1..n, fn &amp;#40;&amp;#95;&amp;#41; -&amp;gt; receive do x -&amp;gt; x end end
  end

  def run&amp;#40;m, n&amp;#41; do
    IO.puts inspect :timer.tc&amp;#40;Chain, :test, &amp;#91;m, n&amp;#93;&amp;#41;
  end
end
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The &lt;code&gt;test&lt;/code&gt; function creates &lt;code&gt;m&lt;/code&gt; Erlang processes each running in a loop receiving messages and sending them on to the next process (the last process receiving the message is the main process itself).  Then it asynchronously sends &lt;code&gt;n&lt;/code&gt; messages to the &lt;code&gt;start&lt;/code&gt; process, and finally it waits for all &lt;code&gt;n&lt;/code&gt; messages to arrive back.&lt;/p&gt;&lt;p&gt;The test with 10000 processes sending around 10000 messages can be executed by running the following command:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;elixir --erl &amp;quot;+P 10000&amp;quot; -r chain.exs -e &amp;quot;Chain.run&amp;#40;10000, 10000&amp;#41;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;On my machine it prints &lt;code&gt;{5877968, :ok}&lt;/code&gt;, meaning that everything went well and the test took about 5.9 seconds.  During this time, all CPU cores had 100% load.&lt;/p&gt;&lt;h2 id=&quot;clojure&amp;#95;agents&amp;#95;solution&quot;&gt;Clojure agents solution&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;&amp;#40;ns chain.agents
  &amp;#40;:import &amp;#91;java.util.concurrent SynchronousQueue&amp;#93;&amp;#41;&amp;#41;

&amp;#40;defn relay &amp;#91;s m&amp;#93;
  &amp;#40;if &amp;#40;instance? clojure.lang.Agent s&amp;#41;
    &amp;#40;send s relay m&amp;#41;
    &amp;#40;.put &amp;#94;SynchronousQueue s m&amp;#41;&amp;#41;
  s&amp;#41;

&amp;#40;defn create-senders &amp;#91;m start&amp;#93;
  &amp;#40;reduce &amp;#40;fn &amp;#91;next &amp;#95;&amp;#93; &amp;#40;agent next&amp;#41;&amp;#41; start &amp;#40;range &amp;#40;dec m&amp;#41;&amp;#41;&amp;#41;&amp;#41;

&amp;#40;defn run &amp;#91;m n&amp;#93;
  &amp;#40;let &amp;#91;message 0
        q &amp;#40;SynchronousQueue.&amp;#41;
        start &amp;#40;create-senders m &amp;#40;agent q&amp;#41;&amp;#41;&amp;#93;
    &amp;#40;dotimes &amp;#91;&amp;#95; n&amp;#93;
      &amp;#40;send start relay message&amp;#41;&amp;#41;
    &amp;#40;dotimes &amp;#91;&amp;#95; n&amp;#93;
      &amp;#40;.take q&amp;#41;&amp;#41;&amp;#41;&amp;#41;

&amp;#40;defn -main &amp;#91;m n&amp;#93;
  &amp;#40;time &amp;#40;run &amp;#40;Integer/parseInt m&amp;#41; &amp;#40;Integer/parseInt n&amp;#41;&amp;#41;&amp;#41;
  &amp;#40;shutdown-agents&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here we create &lt;code&gt;m&lt;/code&gt; agents linked to each other, except that the first agent gets a queue as its state.  Then, just like in the Elixir version, &lt;code&gt;n&lt;/code&gt; messages are sent asynchronously to the start agent and finally the main thread takes the messages from the queue.&lt;/p&gt;&lt;p&gt;After running &lt;code&gt;lein uberjar&lt;/code&gt;, the program can be executed with&lt;/p&gt;&lt;pre&gt;&lt;code&gt;java -cp target/chain-standalone.jar clojure.main -m chain.agents 10000 10000
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and prints &lt;code&gt;&amp;quot;Elapsed time: 89672.860248 msecs&amp;quot;&lt;/code&gt;.  The first thing that leaps to the eye is that this is about 15 times slower than the Elixir version.  The second thing is that the CPU usage is about 20% on all cores.  The latter suggest, that we might have some problems with scheduling the work, so let's tune that a bit.&lt;/p&gt;&lt;h3 id=&quot;tuning&amp;#95;agent&amp;#95;action&amp;#95;scheduling&quot;&gt;Tuning agent action scheduling&lt;/h3&gt;&lt;p&gt;The &lt;code&gt;send&lt;/code&gt; function dispatches actions to the agents using a Java fixed thread pool executor with pool size set to the number of available (logical) processors plus two.  This default executor can be overwritten by the &lt;code&gt;set-agent-send-executor!&lt;/code&gt; function.  By changing &lt;code&gt;-main&lt;/code&gt; we can experiment with various pool sizes:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&amp;#40;ns chain.agents
  &amp;#40;:import &amp;#91;java.util.concurrent SynchronousQueue Executors&amp;#93;&amp;#41;&amp;#41;

;; functions before -main unchanged

&amp;#40;defn -main &amp;#91;p m n&amp;#93;
  &amp;#40;set-agent-send-executor! &amp;#40;Executors/newFixedThreadPool &amp;#40;Integer/parseInt p&amp;#41;&amp;#41;&amp;#41;
  &amp;#40;time &amp;#40;run &amp;#40;Integer/parseInt m&amp;#41; &amp;#40;Integer/parseInt n&amp;#41;&amp;#41;&amp;#41;
  &amp;#40;shutdown-agents&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It turns out, that on my machine with eight logical processors the program runs fastest with three threads in the thread pool: &lt;code&gt;&amp;quot;Elapsed time: 61878.63678 msecs&amp;quot;&lt;/code&gt;.  This clearly indicates that more threads just hinder each other in doing useful work.&lt;/p&gt;&lt;p&gt;It is a bit annoying that we cannot use all processors, but fortunately, since Java 1.8 the concurrency framework provides a work stealing pool which works in a way very similar to the Erlang scheduler.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;;; everything before -main unchanged

&amp;#40;defn -main &amp;#91;m n&amp;#93;
  &amp;#40;set-agent-send-executor! &amp;#40;Executors/newWorkStealingPool&amp;#41;&amp;#41;
  &amp;#40;time &amp;#40;run &amp;#40;Integer/parseInt m&amp;#41; &amp;#40;Integer/parseInt n&amp;#41;&amp;#41;&amp;#41;
  &amp;#40;shutdown-agents&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This version prints &lt;code&gt;&amp;quot;Elapsed time: 35880.203266 msecs&amp;quot;&lt;/code&gt; and produces 100% CPU usage.  It seems, scheduling is not the bottleneck anymore. How does the work stealing pool do this?&lt;/p&gt;&lt;p&gt;The main difference between the work stealing and the fixed thread pool is, that the latter has a single queue of tasks from which the worker threads can take tasks, and where new tasks can be put.  At high levels of parallelism this results in contention.  The work stealing pool uses several queues which are dedicated to threads and when a thread produces work, it is put the queue of thread.  Only when the queue of a thread is empty does the thread go to see if it can steal tasks from the queue of another thread.  This means, that as long as the threads produce enough work for themselves (which the agents in our example do), there will be no contention and no waiting at all.&lt;/p&gt;&lt;p&gt;This is all very nice, but the Clojure agents are still about seven times slower than the Elixir/Erlang processes.  Looking at the hot spots in Java VisualVM reveals that the function &lt;code&gt;push-thread-bindings&lt;/code&gt; used for binding the dynamic &lt;code&gt;&amp;#42;agent&amp;#42;&lt;/code&gt; variable to the agent we are sending work to is responsible for about 20% of the runtime.  Let's have a look at a solution that doesn't have this overhead.&lt;/p&gt;&lt;h2 id=&quot;clojure&amp;#95;core.async&amp;#95;solution&quot;&gt;Clojure core.async solution&lt;/h2&gt;&lt;p&gt;In this solution we use core.async go blocks to represent relays.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&amp;#40;ns chain.async
  &amp;#40;:require &amp;#91;clojure.core.async :as async :refer &amp;#91;&amp;lt;! &amp;gt;! &amp;lt;!!&amp;#93;&amp;#93;&amp;#41;&amp;#41;

&amp;#40;defn relay &amp;#91;in out&amp;#93;
  &amp;#40;async/go-loop &amp;#91;&amp;#93;
    &amp;#40;&amp;gt;! out &amp;#40;&amp;lt;! in&amp;#41;&amp;#41;
    &amp;#40;recur&amp;#41;&amp;#41;&amp;#41;

&amp;#40;defn create-senders &amp;#91;m start&amp;#93;
  &amp;#40;reduce &amp;#40;fn &amp;#91;in &amp;#95;&amp;#93; &amp;#40;let &amp;#91;out &amp;#40;async/chan&amp;#41;&amp;#93; &amp;#40;relay in out&amp;#41; out&amp;#41;&amp;#41;
          start &amp;#40;range m&amp;#41;&amp;#41;&amp;#41;

&amp;#40;defn run &amp;#91;m n&amp;#93;
  &amp;#40;let &amp;#91;message 0
        start &amp;#40;async/chan&amp;#41;
        end &amp;#40;create-senders m start&amp;#41;&amp;#93;
    &amp;#40;async/go
      &amp;#40;dotimes &amp;#91;&amp;#95; n&amp;#93;
        &amp;#40;&amp;gt;! start message&amp;#41;&amp;#41;&amp;#41;
    &amp;#40;dotimes &amp;#91;&amp;#95; n&amp;#93;
      &amp;#40;&amp;lt;!! end&amp;#41;&amp;#41;&amp;#41;&amp;#41;

&amp;#40;defn -main &amp;#91;m n&amp;#93;
  &amp;#40;time &amp;#40;run &amp;#40;Integer/parseInt m&amp;#41; &amp;#40;Integer/parseInt n&amp;#41;&amp;#41;&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The structure of the program is the same as before, but here we have to generate channels to be able to communicate between processes. Executing this program with&lt;/p&gt;&lt;pre&gt;&lt;code&gt;java -cp target/chain-standalone.jar clojure.main -m chain.async 10000 10000
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;we get &lt;code&gt;Elapsed time: 81157.570264 msecs&amp;quot;&lt;/code&gt; and 40% CPU usage on all cores.&lt;/p&gt;&lt;p&gt;We seem to have a scheduling problem again.  But before looking for ways to override the executor again (which is not supported out of the box, see &lt;a href='https://dev.clojure.org/jira/browse/ASYNC-94'&gt;ASYNC-94&lt;/a&gt;) let's compare this solution and the Elixir one.&lt;/p&gt;&lt;p&gt;Both solutions create processes for relaying the messages, but only the Elixir solution is truly asynchronous.  In Elixir, each process has a mailbox where the messages sent to it land and the sender doesn't wait until the addressee pulls the message.  In the core.async solution the processes have to synchronise, as the channels have no buffers.  Let's see what happens if we decouple the processes a bit.&lt;/p&gt;&lt;h3 id=&quot;tuning&amp;#95;core.async&amp;#95;buffering&quot;&gt;Tuning core.async buffering&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;&amp;#40;defn create-senders &amp;#91;m start&amp;#93;
  &amp;#40;reduce &amp;#40;fn &amp;#91;in &amp;#95;&amp;#93; &amp;#40;let &amp;#91;out &amp;#40;async/chan 1000&amp;#41;&amp;#93; &amp;#40;relay in out&amp;#41; out&amp;#41;&amp;#41;
          start &amp;#40;range m&amp;#41;&amp;#41;&amp;#41;

&amp;#40;defn run &amp;#91;m n&amp;#93;
  &amp;#40;let &amp;#91;message 0
        start &amp;#40;async/chan 1000&amp;#41;
        end &amp;#40;create-senders m start&amp;#41;&amp;#93;
    &amp;#40;async/go
      &amp;#40;dotimes &amp;#91;&amp;#95; n&amp;#93;
        &amp;#40;&amp;gt;! start message&amp;#41;&amp;#41;&amp;#41;
    &amp;#40;dotimes &amp;#91;&amp;#95; n&amp;#93;
      &amp;#40;&amp;lt;!! end&amp;#41;&amp;#41;&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now a go block can send 1000 messages without waiting for the other end to handle them.  When executed, the program prints &lt;code&gt;Elapsed time: 7157.570264 msecs&amp;quot;&lt;/code&gt; and the CPU usage goes up to 100% on all cores.  This is comparable to the Elixir version. Also, the VisualVM sampler shows that the time is spent in the put and take operations, which is as expected.&lt;/p&gt;&lt;h2 id=&quot;bigger&amp;#95;messages&quot;&gt;Bigger messages&lt;/h2&gt;&lt;p&gt;One aspect of this benchmark that's quite artificial is that the messages are small integers.  Let's see what happens if instead of &lt;code&gt;0&lt;/code&gt; we send around a list of twenty integers.  This is how the Elixir version looks like:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;  def test&amp;#40;m, n&amp;#41; do
    message = Enum.to&amp;#95;list 1..20 # &amp;lt;- this is the only change
    start = create&amp;#95;senders&amp;#40;m&amp;#41;
    Enum.each 1..n, fn &amp;#40;&amp;#95;&amp;#41; -&amp;gt; send start, message end
    Enum.each 1..n, fn &amp;#40;&amp;#95;&amp;#41; -&amp;gt; receive do x -&amp;gt; x end end
  end
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And this is the equivalent change for the core.async version:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&amp;#40;defn run &amp;#91;m n&amp;#93;
  &amp;#40;let &amp;#91;message &amp;#40;into &amp;#40;&amp;#41; &amp;#40;range 20&amp;#41;&amp;#41; ; &amp;lt;- this is the only change
        start &amp;#40;async/chan 1000&amp;#41;
        end &amp;#40;create-senders m start&amp;#41;&amp;#93;
    &amp;#40;async/go
      &amp;#40;dotimes &amp;#91;&amp;#95; n&amp;#93;
        &amp;#40;&amp;gt;! start message&amp;#41;&amp;#41;&amp;#41;
    &amp;#40;dotimes &amp;#91;&amp;#95; n&amp;#93;
      &amp;#40;&amp;lt;!! end&amp;#41;&amp;#41;&amp;#41;&amp;#41;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This change more than doubles the runtime of the Elixir version: &lt;code&gt;{16641097, :ok}&lt;/code&gt;, while the runtime of the Clojure versions remains roughly unchanged: &lt;code&gt;&amp;quot;Elapsed time: 9755.010961 msecs&amp;quot;&lt;/code&gt;. This is because Erlang processes share nothing, and the message is copied each time it is sent.  In Clojure there is a global heap, so only a reference to the list has to be sent around.  Why does the Erlang virtual machine do this copying?  After all, Erlang lists are immutable so sharing them should not be dangerous.  The reason is that not sharing data enables the Erlang VM to maintain separate heaps for the individual processes which means these heaps stay small and can be garbage collected separately.  This in turn results in much shorter and more predictable GC related latencies.&lt;/p&gt;&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;&lt;p&gt;I compared asynchronous message passing performance of Elixir and Clojure, in Clojure I looked at both agents and core.async.  I discussed issues related to scheduling, buffering and garbage collection.  The following points seem to be the most important takeaways:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;The Erlang process scheduler and message passing system is an   impressive piece of work, and offers excellent out of the box   experience.&lt;/li&gt;&lt;li&gt;In Clojure, agents are not the way to go if message passing   performance is vital.&lt;/li&gt;&lt;li&gt;Clojure's core.async library offers similar message passing   performance as Erlang, especially if the messages are bigger than   just a couple of small numbers and buffering can be tweaked.&lt;/li&gt;&lt;li&gt;Erlang's GC probably causes smaller latencies than the default JVM   garbage collector.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Feel free to experiment with the &lt;a href='https://github.com/bentomi/message-passing-test'&gt;code&lt;/a&gt; yourself.&lt;/p&gt;
</description>
<pubDate>
Sun, 04 Jun 2017 00:00:00 +0200
</pubDate>
</item>
</channel>
</rss>
